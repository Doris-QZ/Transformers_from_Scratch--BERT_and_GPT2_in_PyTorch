{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Doris-QZ/Transformers_from_Scratch--BERT_and_GPT2_in_PyTorch/blob/main/1_Data_For_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "This notebook prepares the **IMDB dataset** for BERT-style pre-training. The data is processed to align with **BERT’s training objectives**:\n",
        "\n",
        "* **Masked Language Modeling (MLM)**: randomly masks tokens for prediction.\n",
        "* **Next Sentence Prediction (NSP)**: create paired sentences with both positive and negative examples.  \n",
        ">\n",
        "\n",
        "At the end of this notebook, we'll have a pandas DataFrame with the following columns:\n",
        "* **input_ids** - token IDs of paired sentences       \n",
        "* **token_type_ids** - token type IDs (0 for the first sentence, 1 for the second)\n",
        "* **attention_mask** - 1 for real tokens, 0 for padding\n",
        "* **mlm_labels** - labels for masked language model prediction\n",
        "* **nsp_labels** - labels for next sentence prediction.  \n",
        ">\n",
        "   \n",
        "Although this is not the same corpus used in the original BERT paper (BooksCorpus + Wikipedia), the dataset is structured in a way that allows the model to be trained with the same objectives.  \n"
      ],
      "metadata": {
        "id": "EE8tNqYC23Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import pandas as pd\n",
        "import random"
      ],
      "metadata": {
        "id": "xKa-gL4Z4_q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress the warning\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# Load the data\n",
        "imdb_data = load_dataset('imdb', split='train')"
      ],
      "metadata": {
        "id": "kQwTagUg6gvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the data\n",
        "print(f\"{imdb_data}\\n\")\n",
        "imdb_data[0]"
      ],
      "metadata": {
        "id": "P1hngl8m6HrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT uses **WordPiece**, a subword tokenization algorithm, with a vocabulary size of 30,522 in the BERT-base model. We’ll load the BERT-base tokenizer from Hugging Face’s transformers package for text tokenization.\n",
        ">\n",
        "**Note:** Although the BERT tokenizer can directly return `input_ids`, `attention_mask`, and `token_type_ids`, in this project we only use its `encode` method to obtain token IDs. We will then **create `input_ids`, `attention_mask`, and `token_type_ids` manually**, since our goal is to **explore how data is prepared for masked language modeling (MLM) and next sentence prediction (NSP)**."
      ],
      "metadata": {
        "id": "qWgSMxei1ZQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT-base tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Get vocabulary and vocabulary size\n",
        "vocab = bert_tokenizer.get_vocab()\n",
        "vocab_size = len(vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "EcMedaBVGmYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for Preparing Data for Masked Language Modeling (MLM)\n",
        "According to the BERT paper, **15% of tokens are randomly selected for masking**.  \n",
        "\n",
        "For example,<br>\n",
        "\n",
        "> Original text: \"my dog is very cute.\"  \n",
        "> Selected token: \"cute\"\n",
        "\n",
        "For the selected tokens:\n",
        "* 80% of time ---> replaced with [MASK] ---> \"my dog is very **[MASK]**.\"\n",
        "* 10% of time ---> left unchanged ---> \"my dog is very **cute**.\"\n",
        "* 10% of time ---> replaced with a random token ---> \"my dog is very **apple**.\"    \n",
        ">\n",
        "\n",
        "Regardless of how the selected tokens are masked, the label remains the same:\n",
        ">[PAD][PAD][PAD][PAD] cute.  \n",
        "\n",
        "During training, the model **ignores the [PAD] tokens** and predicts the orginal tokens at the selected positions."
      ],
      "metadata": {
        "id": "5SyaYdlqLoX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get special token indices\n",
        "PAD_IDX = bert_tokenizer.pad_token_id\n",
        "UNK_IDX = bert_tokenizer.unk_token_id\n",
        "CLS_IDX = bert_tokenizer.cls_token_id\n",
        "SEP_IDX = bert_tokenizer.sep_token_id\n",
        "MASK_IDX = bert_tokenizer.mask_token_id\n",
        "\n",
        "PAD_IDX, UNK_IDX, CLS_IDX, SEP_IDX, MASK_IDX"
      ],
      "metadata": {
        "id": "oMSEdHmwvbjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_ids = {PAD_IDX, UNK_IDX, CLS_IDX, SEP_IDX, MASK_IDX}\n",
        "\n",
        "def random_token(vocab_size=vocab_size, special_ids=special_ids):\n",
        "    \"\"\"\n",
        "    Returns a random token ID that is not a special token.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        idx = random.randint(0, vocab_size-1)\n",
        "        if idx not in special_ids:\n",
        "            return idx\n",
        "\n",
        "def masking(token_id, PAD_IDX, MASK_IDX):\n",
        "    \"\"\"\n",
        "    Mask a single token for Masked Language Model (MLM) training.\n",
        "\n",
        "    Args:\n",
        "        token_id (int): The token_id to be processed.\n",
        "        valid_tokens (list): A list of tokens to get random tokens from.\n",
        "\n",
        "    Returns:\n",
        "        tuple of two str---\n",
        "            1. The processed token id, which may be replaced with MASK_IDX, left unchanged,\n",
        "            or replaced with a random token id.\n",
        "            2. The label for the token-the original token id if masked, or PAD_IDX if not masked.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # The probability of a token being masked is 15%.\n",
        "    mask = random.random() <= 0.15\n",
        "\n",
        "    if not mask:\n",
        "        token_ = token_id\n",
        "        label_ = PAD_IDX\n",
        "        return token_, label_\n",
        "\n",
        "    # Generates a random float between 0 and 1\n",
        "    random_float = random.random()\n",
        "\n",
        "    # 80% of the selected tokens will be repalced by MASK_IDX\n",
        "    if random_float < 0.8:\n",
        "        token_ = MASK_IDX\n",
        "        label_ = token_id\n",
        "        return token_, label_\n",
        "\n",
        "    # 10% of the selected tokens will remain unchanged\n",
        "    if random_float > 0.9:\n",
        "        token_ = token_id\n",
        "        label_ = token_id\n",
        "        return token_, label_\n",
        "\n",
        "    # 10% of the selected tokens will be replaced by a random token id\n",
        "    else:\n",
        "        token_ = random_token()\n",
        "        label_ = token_id\n",
        "        return token_, label_"
      ],
      "metadata": {
        "id": "t5_R9cbvLVXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PUNCT_IDX = [vocab['.'], vocab['?'], vocab['!']]\n",
        "\n",
        "def data_for_MLM(dataset, pad_idx, mask_idx, punct_idx):\n",
        "    \"\"\"\n",
        "    Prepare data for Masked Language model (MLM) training.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): The dataset to be processed.\n",
        "        pad_idx (int): The index of the [PAD] token.\n",
        "        mask_idx(int): The index of the [MASK] token.\n",
        "        punct_idx(list): A list of indices of the ending punctuations\n",
        "\n",
        "    Returns:\n",
        "        tuple of two lists--\n",
        "            1. List of tokenized sentences, where each token is either replaced with MASK_IDX,\n",
        "            left unchanged, or replaced with a random token.\n",
        "            2. List of labels for masked tokens corresponding to the tokenized sentences.\n",
        "            Each label is either the original token at masked positions or PAD_IDX at unmasked positions.\n",
        "    \"\"\"\n",
        "\n",
        "    masked_sentences = []\n",
        "    mlm_labels = []\n",
        "    cur_tokens = []\n",
        "    cur_labels = []\n",
        "\n",
        "\n",
        "    for data in dataset:\n",
        "        tokens =  bert_tokenizer.encode(data['text'])\n",
        "\n",
        "        for token_id in tokens:\n",
        "            token_, label_ = masking(token_id, PAD_IDX, MASK_IDX)\n",
        "            cur_tokens.append(token_)\n",
        "            cur_labels.append(label_)\n",
        "\n",
        "            # Found a token indicates the end of sentence, process the sentence and reset it.\n",
        "            if token_id in punct_idx:\n",
        "                if len(cur_tokens) > 2:\n",
        "                    masked_sentences.append(cur_tokens)\n",
        "                    mlm_labels.append(cur_labels)\n",
        "                    cur_tokens = []\n",
        "                    cur_labels = []\n",
        "                else:\n",
        "                    cur_tokens = []\n",
        "                    cur_labels = []\n",
        "\n",
        "        # Append the remaining tokens that do not have an ending punctuation to the list\n",
        "        if cur_tokens:\n",
        "            masked_sentences.append(cur_tokens)\n",
        "            mlm_labels.append(cur_labels)\n",
        "\n",
        "    return masked_sentences, mlm_labels\n"
      ],
      "metadata": {
        "id": "IeN1Rf9XFwWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for Preparing Data for Next Sentence Prediction (NSP)\n",
        "\n",
        "BERT is trained to predict whether a pair of sentences are consecutive in the original text (**Next Sentence Prediction**).\n",
        "\n",
        "* **Positive examples**: the second sentence follows the first sentence in the dataset.\n",
        "\n",
        "* **Negative examples**: the second sentence is randomly selected from the dataset.\n",
        "\n",
        "For each sentence pair, a label is created:\n",
        "\n",
        "* 1 → Next sentence is correct (positive example)\n",
        "\n",
        "* 0 → Next sentence is incorrect (negative example)\n",
        "\n",
        "This allows the model to learn relationships between sentences in addition to word-level predictions."
      ],
      "metadata": {
        "id": "FFDpmLKYFbCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_for_NSP(masked_sentences, mlm_labels, pad_idx, cls_idx, sep_idx):\n",
        "    \"\"\"\n",
        "    Prepare data for Next Sentence Prediction (NSP).\n",
        "\n",
        "    Args:\n",
        "        masked_sentences (list): List of tokenized sentences\n",
        "        mlm_labels (list): List of labels corresponding to input_tokens.\n",
        "        pad_idx (int): The index of the [PAD] token.\n",
        "        cls_idx(int): The index of the [CLS] token.\n",
        "        sep_idx(int): The index of the [SEP] token.\n",
        "\n",
        "    Returns:\n",
        "        tuple of three lists---\n",
        "            1. List of paired sentences with special token indices added.\n",
        "            2. List of labels for masked token.\n",
        "            3. List of boolean values for Next Sentence Prediction.\n",
        "    \"\"\"\n",
        "\n",
        "    # Make sure the length of inputs are valid\n",
        "    num_sentence = len(masked_sentences)\n",
        "    if num_sentence < 2:\n",
        "        raise ValueError(\"Must be more than two sentences in the input_tokens.\")\n",
        "\n",
        "    if num_sentence != len(mlm_labels):\n",
        "        raise ValueError(\"The input_tokens and the labels must have the same length.\")\n",
        "\n",
        "\n",
        "    paired_sents = []\n",
        "    paired_mlm_labels = []\n",
        "    nsp_labels = []\n",
        "\n",
        "    # Create the list of sentence indices\n",
        "    sentence_idx = list(range(num_sentence))\n",
        "\n",
        "    while len(sentence_idx) >= 2:\n",
        "        if random.random() >= 0.5:\n",
        "            # Randomly choose an index from the sentence_idx\n",
        "            idx = random.choice(sentence_idx[:-1])\n",
        "\n",
        "            # Pair two consecutive sentences (idx, idx+1) with special tokens as current inputs/lables\n",
        "            cur_input = [[CLS_IDX] + masked_sentences[idx] + [SEP_IDX],\n",
        "                          masked_sentences[idx + 1] + [SEP_IDX]]\n",
        "            cur_label = [[PAD_IDX] + mlm_labels[idx] + [PAD_IDX],\n",
        "                          mlm_labels[idx + 1] + [PAD_IDX]]\n",
        "\n",
        "            # Add current inputs/labels to paired_inputs/paired_labels\n",
        "            paired_sents.append(cur_input)\n",
        "            paired_mlm_labels.append(cur_label)\n",
        "\n",
        "            # Append 1 to nsp_label, indicating the current inputs are consecutive sentences.\n",
        "            nsp_labels.append(1)\n",
        "\n",
        "            # Remove idx and idx+1 from sentence_idx\n",
        "            sentence_idx.remove(idx)\n",
        "            if idx + 1 in sentence_idx:\n",
        "                sentence_idx.remove(idx+1)\n",
        "\n",
        "        else:\n",
        "            # Randomly sample two indices from the sentence_idx\n",
        "            idx_1, idx_2 = random.sample(sentence_idx, 2)\n",
        "\n",
        "            # Add two randomly selected sentences (idx_1, idx_2) with special tokens as current inputs/lables\n",
        "            cur_input = [[CLS_IDX] + masked_sentences[idx_1] + [SEP_IDX],\n",
        "                         masked_sentences[idx_2] + [SEP_IDX]]\n",
        "            cur_label = [[PAD_IDX] + mlm_labels[idx_1] + [PAD_IDX],\n",
        "                         mlm_labels[idx_2] + [PAD_IDX]]\n",
        "\n",
        "            # Add current inputs/labels to paired_inputs/paired_labels\n",
        "            paired_sents.append(cur_input)\n",
        "            paired_mlm_labels.append(cur_label)\n",
        "\n",
        "            # Append 0 to nsp_label, indicating the current inputs are not consecutive sentences.\n",
        "            nsp_labels.append(0)\n",
        "\n",
        "            # Remove idx_1 and idx_2 from sentence_idx\n",
        "            sentence_idx.remove(idx_1)\n",
        "            sentence_idx.remove(idx_2)\n",
        "\n",
        "    return paired_sents, paired_mlm_labels, nsp_labels"
      ],
      "metadata": {
        "id": "swNZTkrClkgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to Create Final BERT Training Data\n",
        "\n",
        "The final function combines the **MLM** and **NSP** data preparation steps:\n",
        "\n",
        "1. Randomly masks 15% of tokens and prepares the corresponding labels for **Masked Language Modeling (MLM)**.\n",
        "\n",
        "2. Generates sentence pairs and labels for **Next Sentence Prediction (NSP)**.\n",
        "\n",
        "3. Returns a **pandas DataFrame** containing all the processed data.\n",
        ">\n",
        "\n",
        "The output DataFrame can then be saved as a CSV file, which is later loaded in the `Reproducing BERT Model from Scratch using PyTorch.ipynb` notebook. In that notebook, the data is **converted to a PyTorch dataset** and loaded into a **PyTorch DataLoader** for training."
      ],
      "metadata": {
        "id": "jUBXZJdt1ULt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_for_BERT(dataset, pad_idx, mask_idx, cls_idx, sep_idx, punct_idx):\n",
        "    \"\"\"\n",
        "    Prepare data for BERT training--Masked Language Modeling and Next Sentence Prediction.\n",
        "\n",
        "    Args:\n",
        "        dataset(Dataset): The dataset to be processed.\n",
        "        pad_idx (int): The index of the [PAD] token.\n",
        "        mask_idx(int): The index of the [MASK] token.\n",
        "        cls_idx(int): The index of the [CLS] token.\n",
        "        sep_idx(int): The index of the [SEP] token.\n",
        "        punct_idx(list): A list of indices of the ending punctuations\n",
        "\n",
        "    Returns:\n",
        "        A Pandas dataframe with the following columns:\n",
        "            input_ids - token IDs of paired sentences\n",
        "            token_type_ids - token type IDs (0 for the first sentence, 1 for the second)\n",
        "            attention_mask - 1 for real tokens, 0 for padding\n",
        "            mlm_labels - labels for masked language model prediction\n",
        "            nsp_labels - labels for next sentence prediction\n",
        "\n",
        "    \"\"\"\n",
        "    # Pad the paired inputs and flatten the nested list\n",
        "    def pad_flatten(pairs, padding=PAD_IDX):\n",
        "        max_len = max(len(pairs[0]), len([pairs[1]]))\n",
        "        pairs[0].extend([padding] * (max_len - len(pairs[0])))\n",
        "        pairs[1].extend([padding] * (max_len - len(pairs[1])))\n",
        "        flatten_pairs = [item for sublist in pairs for item in sublist]\n",
        "        return flatten_pairs\n",
        "\n",
        "    # Get masked_sentences and the corresponding labels from the dataset\n",
        "    masked_sentences, mask_labels = data_for_MLM(dataset, pad_idx, mask_idx, punct_idx)\n",
        "\n",
        "    # Get paired_sentences, labels, and nsp_labels list\n",
        "    paired_sentences, paired_mask_labels, nsp_labels = data_for_NSP(masked_sentences, mask_labels, pad_idx, cls_idx, sep_idx)\n",
        "\n",
        "    input_ids, token_type_ids, attention_mask, mlm_labels = [], [], [], []\n",
        "\n",
        "    for sentences, labels in zip(paired_sentences, paired_mask_labels):\n",
        "        # Create token types (0: first sentence, 1: second sentence)\n",
        "        token_type = [[0] * len(sentences[0]), [1] * len(sentences[1])]\n",
        "        token_type = pad_flatten(token_type)\n",
        "\n",
        "        # Create attention mask (1: real tokens, 0: padding])\n",
        "        mask = [[1] * len(sentences[0]), [1] * len(sentences[1])]\n",
        "        mask = pad_flatten(mask)\n",
        "\n",
        "        # Pad and flatten paired sentences and mask_labels\n",
        "        padded_sent = pad_flatten(sentences)\n",
        "        padded_label = pad_flatten(labels)\n",
        "\n",
        "        # Convert tokens to indices and add to final lists\n",
        "        input_ids.append(padded_sent)\n",
        "        mlm_labels.append(padded_label)\n",
        "        token_type_ids.append(token_type)\n",
        "        attention_mask.append(mask)\n",
        "\n",
        "    # Create a dataframe of bert data\n",
        "    bert_data = pd.DataFrame({\n",
        "        'input_ids': input_ids,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'mlm_labels': mlm_labels,\n",
        "        'nsp_labels': nsp_labels\n",
        "    })\n",
        "\n",
        "    return bert_data"
      ],
      "metadata": {
        "id": "DLl-LunGw_z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process imdb_data for BERT training\n",
        "imdb_bert_data = data_for_BERT(imdb_data, PAD_IDX, MASK_IDX, CLS_IDX, SEP_IDX, PUNCT_IDX)\n",
        "imdb_bert_data.head()"
      ],
      "metadata": {
        "id": "_YqjQLFwtMPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_bert_data.info()"
      ],
      "metadata": {
        "id": "T0ZcWDYXa0B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dataframe as CSV\n",
        "imdb_bert_data.to_csv('imdb_bert_data.csv', index=False)"
      ],
      "metadata": {
        "id": "QdP5h8tra6yJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}